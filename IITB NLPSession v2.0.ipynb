{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "##   Notebook used for extracting text from html files. Some basic preprocessing tasks \n",
    "##   v2.0 Preprocessing Text   \n",
    "##       - Stopword removal\n",
    "##       - Stemming/ Lemmatization\n",
    "##\n",
    "##   Required packages: os, BeautifulSoup, nltk, ,wordcloud \n",
    "##   The following is used from the nltk packages: corpoira/stopwords, SnowballStemmer, WordNetLemmatizer\n",
    "##\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function created for removing HTML tags from a document \n",
    "def clean_html(htmlDoc):\n",
    "    soup = bs(htmlDoc, 'html.parser') # Parses text so that html tags can be extracted\n",
    "    for script in soup([\"script\", \"style\",\"title\",'[document]', 'head', 'title']):\n",
    "        script.extract() \n",
    "    cleaned=str(soup.get_text(separator=' ').encode('ascii','ignore'))\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read all the html files and open the first speech\n",
    "rootDir = 'E:\\\\NLP Session\\\\RBIGovernorSpeeches\\\\'\n",
    "htmlFiles = [f for f in os.listdir(rootDir) if f.endswith('.html')]\n",
    "\n",
    "fileName = rootDir + htmlFiles[0] \n",
    "cleanedtext = clean_html(open(fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "##  Second paragraph from the speech\n",
    "###########################################################################################################################\n",
    "text1 = 'Over the last few weeks, I have outlined the RBI’s approach to inflation, distressed debt, financial inclusion, banking sector reform, and market reform. Today, I’d like to first discuss why central banking is not as easy as it appears (just raise or cut interest rates!) and why it needs decisions, sometimes unpopular or hard-to-explain ones, to be made under conditions of extreme uncertainty. This will then lead in to my arguments about why we need an independent central bank.'\n",
    "print text1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###########################################################################################################################\n",
    "## Removing stopwords using two methods: \n",
    "## 1. Using nltk corpora of stopwords\n",
    "## 2. Custom stopwords file \n",
    "##\n",
    "###########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "#stopWords = set(stopwords.words('mystopwords'))\n",
    "\n",
    "stopWords = [line.replace('\\n', '') for line in open('stopwords') ]\n",
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Use a lambda function to lower the text, tokenize it and remove it from the corpus if it belongs to the stopwords set \n",
    "##\n",
    "filter(lambda w: not w in stopWords,text1.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## There are some non- alphanumeric tokens in the text, removing them from the corpus\n",
    "##\n",
    "text2 = ''.join(w for w in text1 if (w.isalnum() or w ==' '))\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Removing stopwords from the text corpus\n",
    "##\n",
    "filter(lambda w: not w in stopWords,text2.lower().split())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############################################################################################################################\n",
    "## Object Standardization\n",
    "## Using stemming or lemmatization\n",
    "############################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "clean_text = filter(lambda w: not w in stopWords,text2.lower().split()) ### << Stop word removal\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in clean_text] \n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "clean_text = filter(lambda w: not w in stopWords,text2.lower().split())\n",
    "\n",
    "lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in clean_text] \n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "## Drawing a wordcloud using the wordcloud package\n",
    "##\n",
    "##################################################################################################\n",
    "\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def drawWordcloud(text):\n",
    "    # lower max_font_size\n",
    "    wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "    #wc_array = WordCloud.to_array(text)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileName = rootDir + htmlFiles[0] \n",
    "fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileName = rootDir + htmlFiles[0] \n",
    "text = clean_html(open(fileName))\n",
    "clean_text = filter(lambda w: not w in stopWords,text.lower().split())\n",
    "clean_text = ' '.join(clean_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drawWordcloud(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Running the same code for a different speech\n",
    "##\n",
    "fileName = rootDir + htmlFiles[4] \n",
    "text = clean_html(open(fileName))\n",
    "clean_text = filter(lambda w: not w in stopWords,text.lower().split())\n",
    "clean_text = ' '.join(clean_text)\n",
    "drawWordcloud(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
